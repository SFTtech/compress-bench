#!/usr/bin/python3
#SFT-Tech ultimate compression utility debate resolver
#
#(c) 2013	Michael EnÃŸlin	(michael@ensslin.cc)
#
#License: GPLv3 or higher, no warranty, blabla

import os
import sys
import collections
import argparse
import sqlite3

class resultcache:
	def __init__(self, dbfile):
		#open database connection
		self.con = sqlite3.connect(dbfile)
		self.cur = self.con.cursor()
		#initialize databse
		self.cur.execute("CREATE TABLE IF NOT EXISTS results (srcfile TEXT, util TEXT, ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP, size INT, ctime REAL, dtime REAL, PRIMARY KEY(srcfile, util))")
		self.con.commit()
	
	def write(self, util, ctime, dtime, size):
		ctime = str(ctime)
		dtime = str(dtime)
		size = str(size)
		#try to delete (if exists)
		self.cur.execute("DELETE FROM results WHERE srcfile=? AND util=?", (sourcefile, util))
		#insert new value
		self.cur.execute("INSERT INTO results (srcfile, util, size, ctime, dtime) VALUES (?, ?, ?, ?, ?)", (sourcefile, util, size, ctime, dtime))
		self.con.commit()

	def read(self, util):
		(date, ctime, dtime, size) = self.cur.execute('SELECT ts, ctime, dtime, size FROM results WHERE srcfile=? AND util=?', (sourcefile, util)).fetchone()
		return (date, float(ctime), float(dtime), int(size))
	
def colprint(msg, colcode):
	print("\x1b[" + str(colcode) + "m" + msg + "\x1b[m")

def fatal(msg):
	colprint(msg, 31)
	print_nonfatals("Nonfatal errors before this error:")
	sys.exit(1)

#store all non-fatal errors
nonfatals = []
def print_nonfatals(msg = "Nonfatal errors"):
	if len(nonfatals) > 0:
		print(msg)
		for nonfatal in nonfatals:
			print(nonfatal)

def nonfatal(msg):
	colprint(msg, 33)
	nonfatals.append(msg)

def info(msg):
	colprint(msg, 36)

#parse argv
parser = argparse.ArgumentParser(description='Compression utility benchmark')
parser.add_argument("source", help="this file is used as input for benchmarking, and should contain representative data")
parser.add_argument("dest", help="compressed data is written here, the file must not exist, but the directory must be writable")
parser.add_argument("util", help="compression utility that should be benchmarked (may include arguments such as -1). if none are specified, the default utils are used.", nargs='*')
parser.add_argument("--default-utils", help="Enforce using the default utils, in addition to the manually specified ones", action="store_true", default=False)
parser.add_argument("--csv", help="Store the tables in .csv files in the current working directory", action="store_true", default=False)
parser.add_argument("--rerun", help="Do not use previously cached test results", action="store_true", default=False)
parser.add_argument("--cachefile", help="Specify cache database name", default="compbench_cache.sqlite")
args = parser.parse_args()
sourcefile = os.path.abspath(args.source)
destfile = os.path.abspath(args.dest)
use_cache = not args.rerun
cache = resultcache(args.cachefile)

#compose util list
utils = args.util
if len(utils) == 0 or args.default_utils:
	#default utils
	utils += ["compress", ("zpipe -1", "zpipe -d"), ("zpipe -2", "zpipe -d"), ("zpipe -3", "zpipe -d")]
	for util in ["lzop", "gzip", "bzip2", "lzma", "xz", "xz -e"]:
		for x in range(1, 10):
			utils.append(util + " -" + str(x))

#sourcefile must be a file
if not os.path.isfile(sourcefile):
	if os.path.isdir(sourcefile):
		print("If you wish to benchmark with the content of a directory, run it through tar -c first.")
	fatal(sourcefile + " is not a file")

#destfile must be in a writable directory
destdir = os.path.dirname(destfile)
if not os.path.isdir(destdir):
	fatal("Destination file must be in an existing directory")

if not os.access(destdir, os.W_OK):
	fatal(destdir + " is not writable")

if os.path.isfile(destfile) and not os.access(destfile, os.W_OK):
	fatal(destfile + " is not writable")

#get size of sourcefile
oldsize = os.path.getsize(sourcefile)
if oldsize == 0:
	fatal("Source file must be non-zero sized")

class datarate():
	def __init__(self, dr):
		self.dr = dr

	def __repr__(self):
		def fourdigits(x):
			if x < 10:
				return '{:5.3f}'.format(x)
			elif x < 100:
				return '{:5.2f}'.format(x)
			elif x < 1000:
				return '{:5.1f}'.format(x)
			else:
				return '{:5.0f}'.format(x)

		if self.dr == float("inf"):
			return "+ inf  B/s"
		elif self.dr < 1000:
			return fourdigits(self.dr) + "  B/s"
		elif self.dr < 1000000:
			return fourdigits(self.dr/1000) + " KB/s"
		elif self.dr < 1000000000:
			return fourdigits(self.dr/1000000) + " MB/s"
		else:
			return fourdigits(self.dr/1000000000) + " GB/s"

class percentage():
	def __init__(self, pc):
		self.pc = pc

	def __repr__(self):
		return '{:5.2f}'.format(self.pc)

class utilresult:
	def __init__(self, util, ctime, dtime, size):
		self.util = util
		self.ctime = ctime
		self.dtime = dtime
		self.size = size

		#calculate some more stats
		self.cdmaxtime = max(ctime, dtime)
		self.cdtime = ctime + dtime
		self.diff = oldsize - size
		if self.ctime <= 0:
			self.crate = float("inf")
		else:
			self.crate = oldsize / self.ctime
		if self.dtime <= 0:
			self.drate = float("inf")
		else:
			self.drate = oldsize / self.dtime

		self.percentage = 100 * self.diff / oldsize
		self.outperformedby = None

	#returns most important members as key/value pairs
	def kvpairs(self):
		yield ("Util", self.util)
		yield ("Percentage", percentage(self.percentage))
		yield ("Compression rate", datarate(self.crate))
		yield ("Decompression rate", datarate(self.drate))
		yield ("Outperformed by", self.outperformedby)

	def __repr__(self):
		maxlen = 0
		pairs = []
		for k, v in self.kvpairs():
			if len(str(k)) > maxlen:
				maxlen = len(str(k))
			pairs.append((k, v))

		line = ""
		for k, v in pairs:
			line += str(k).ljust(maxlen + 2) + str(v) + "\n"
		return line[:-1]

	#duration for sequential operation with given transmission rate and transmission count
	def time_seq(self, rate, k):
		return self.cdtime + k * self.size / rate

	#check whether this outperforms other in all respects 
	def outperforms(self, other):
		if self.size < other.size and self.ctime < other.ctime and self.dtime < other.dtime:
			return True
		else:
			return False
			
#monotonic clock code
import ctypes
__all__ = ["monotonic_time"]
CLOCK_MONOTONIC = 1 # see <linux/time.h>
class timespec(ctypes.Structure):
    _fields_ = [
        ('tv_sec', ctypes.c_long),
        ('tv_nsec', ctypes.c_long)
    ]

librt = ctypes.CDLL('librt.so.1', use_errno=True)
clock_gettime = librt.clock_gettime
clock_gettime.argtypes = [ctypes.c_int, ctypes.POINTER(timespec)]

#returns monotonic time, as float, in seconds
def monotonic_time():
    t = timespec()
    if clock_gettime(CLOCK_MONOTONIC, ctypes.pointer(t)) != 0:
        errno_ = ctypes.get_errno()
        raise OSError(errno_, os.strerror(errno_))
    return t.tv_sec + t.tv_nsec * 1e-9

#returns runtime of command, as float, in seconds
def timecommand(command):
	print(command)
	t_begin = monotonic_time()
	exitcode = os.system(command)
	t_end = monotonic_time()

	if exitcode == 0:
		return t_end - t_begin
	else:
		nonfatal("command \"" + command + "\" failed")
		return None

def dobench(util, sourcefile, destfile):
	if type(util) == tuple:
		comp = util[0]
		decomp = util[1]
	else:
		comp = util
		decomp = util + " -d"
	info("Testing " + comp)
	if use_cache:
		try:
			#check for cached results
			(date, ctime, dtime, size) = cache.read(comp)
			info("Using cached results from " + date)
			return utilresult(comp, ctime, dtime, size)
		except:
			pass
	#measure compression times
	ctime = timecommand("pv '" + sourcefile + "' | " + comp + " > '" + destfile + "'")
	if ctime != None:
		dtime = timecommand("pv '" + destfile + "' | " + decomp + " > /dev/null")
	if ctime == None or dtime == None:
		nonfatal("Measurement for util " + comp + " failed")
		return None
	else:
		size = os.path.getsize(destfile)
		cache.write(comp, ctime, dtime, size)
		#return the measurement result object
		return utilresult(comp, ctime, dtime, size)

#add dummy result for 'no compression'
results = [utilresult("cat", 0, 0, oldsize)]
for r in map(lambda util: dobench(util, sourcefile, destfile), utils):
	if r == None:
		continue

	#check whether this result outperforms/is outperformed by any other existing result
	for other in results:
		if r.outperforms(other):
			#r is better than other in all aspects
			other.outperformedby = r.util

		if other.outperforms(r):
			#r is worse than other in all aspects
			r.outperformedby = other.util

	#append result to result list
	results.append(r)

	#print result immediately, for the impatient users
	print(r)

print("Processing measurement results...")

#sort the results by their achieved compression rates
results = sorted(results, key=lambda r: r.percentage)

#takes a list of unsorted points, and returns a yields interval tuples in a sorted order.
#the interval tuples consist of (start, middle, end).
#the last interval tuple will be (start, +inf, +inf).
def list_to_intervals(l):
	lastpoint = None
	for point in sorted(l) + [float("inf")]:
		if lastpoint == point:
			continue
		if lastpoint != None:
			yield (lastpoint, (point + lastpoint)/2, point)
		lastpoint = point

#yields (datarate, best util) tuples
#intersectfun must return a list of (possible) intersections
#between the time curves of two utilresults
#timefun must return the time for a given rate
def getbestutils(intersectfun, timefun):
	#find the util that has the highest compression (rate = 0)
	curbest = results[0]
	for r in results:
		if(r.size < curbest.size):
			curbest = r
	yield (0, curbest)

	intersections = []
	for r0 in results:
		for r1 in results:
			inters = intersectfun(r0, r1)
			if type(inters) == list:
				intersections += inters
			else:
				intersections.append(inters)

	#begin is the rate at the beginning of the rate interval,
	#middle is the rate in the middle of the interval
	#we do not need the rate at the end.
	for begin, middle, _ in list_to_intervals(intersections):
		#ignore intersections at negative rates
		if begin < 0:
			continue

		oldbest = curbest
		for r in results:
			#check whether r is better than curbest in the
			#given interval
			if timefun(r, middle) < timefun(curbest, middle):
				curbest = r
				
		#check whether curbest has changed
		if curbest != oldbest:
			yield (begin, curbest)

class resulttable:
	def __init__(self):
		self.data = {}
		self.rows = collections.OrderedDict()
		self.cols = collections.OrderedDict()

	def insval(self, row, col, v):
		if v == None:
			return

		#update column and row indices with their maximum field size values
		if not row in self.rows:
			self.rows[row] = max(len(str(row)), len(str(v)))
			self.data[row] = collections.OrderedDict()
		else:
			self.rows[row] = max(self.rows[row], len(str(v)))

		if not col in self.cols:
			self.cols[col] = max(len(str(col)), len(str(v)))
		else:
			self.cols[col] = max(self.cols[col], len(str(v)))

		#store the value
		self.data[row][col] = v

	def getval(self, row, col):
		if not row in self.rows or not col in self.data[row]:
			return None
		else:
			return self.data[row][col]

	def getvalstr(self, row, col, printnone = "none"):
		val = self.getval(row, col)
		if val == None:
			return printnone
		else:
			return str(val)

	def getline(self, source, sep, padding):
		line = ""
		for col in self.cols:
			data = str(source(col))
			if padding:
				line += data.ljust(self.cols[col])
			else:
				line += data
			line += sep

		return line[:-len(sep)]

	def gettitle(self, sep = " | ", padding = True):
		return self.getline(lambda x: x, sep, padding)

	def getempty(self, sep = " | ", padding = True):
		return self.getline(lambda x: "", sep, padding)

	def getbody(self, printnone = "none", sep = " | ", padding = True):
		body = ""
		for row in self.rows:
			body += self.getline(lambda x: self.getvalstr(row, x, printnone), sep, padding) + "\n"
		return body[:-1]

	def print(self, desc, printnone="none"):
		print(desc + "\n")
		print(self.gettitle())
		print(self.getempty())
		print(self.getbody(printnone))
	
	def to_csv(self, filename):
		f = open(filename, 'w')
		f.write(self.gettitle(",") + "\n")
		f.write(self.getbody("", ",") + "\n")
		f.close()
	
	def sort(self):
		newrows = collections.OrderedDict()
		for r in sorted(self.rows):
			newrows[r] = self.rows[r]
		self.rows = newrows
	
#create table for measurement results
goodtable = resulttable()
badtable = resulttable()
for r in results:
	if(r.outperformedby == None):
		table = goodtable
	else:
		table = badtable
	#insert r into the table
	for k, v in r.kvpairs():
		table.insval(r.util, k, v)
#and print it
goodtable.print("\n\nMeasurement results for compression utilities")
if len(badtable.rows) > 0:
	badtable.print("\nCompression utilities that are worse than some other in all aspects, i.e. completely useless for this workload")

if args.csv:
	goodtable.to_csv("utils.csv")
	badtable.to_csv("utils-outperformed.csv")

#helper functions for find best utils for given datarates
#these functions find intersections between the time(rate) cuves of two
#utils r0 and r1.

def intersects_par(r0, r1):
	#intersections can only occur between saturated and non-saturated
	#lines.

	#r0.cdmaxtime = r1.size/rate
	#r1.cdmaxtime = r0.size/rate
	result = []
	if r0.cdmaxtime > 0:
		result.append(r1.size/r0.cdmaxtime)
	if r1.cdmaxtime > 0:
		result.append(r0.size/r1.cdmaxtime)
	return result

def intersects_seq(r0, r1, k):
	#r0.cdtime + r0.size/rate = r1.cdtime + r1.size/rate
	cdtimediff = r1.cdtime - r0.cdtime
	sizediff = r0.size - r1.size
	#no dividing by zero
	if cdtimediff != 0:
		return k * sizediff / cdtimediff
	else:
		return []

def intersects_dist(r0, r1, k):
	#r0.ctime + (k + 1) * r0.size/rate + k * r0.dtime =
	#r1.ctime + (k + 1) * r1.size/rate + k * r1.dtime
	r0cddtime = r0.ctime + k * r0.dtime
	r1cddtime = r1.ctime + k * r1.dtime
	cddtimediff = r1cddtime - r0cddtime
	sizediff = r0.size - r1.size
	#no dividing by zero
	if cddtimediff != 0:
		return (k + 1) * sizediff / cddtimediff
	else:
		return []

#create table of optimal utils for datarates
optimaltable = resulttable()
def insoptimalutil(rate, col, util):
	optimaltable.insval(rate, "Data rate", ">= " + str(datarate(rate)))
	optimaltable.insval(rate, col, util.util)

for rate, util in getbestutils(intersects_par, lambda r, rate: max(r.cdmaxtime, r.size/rate)):
	insoptimalutil(rate, "PAR", util)

for k in [1, 2]:
	for rate, util in getbestutils(lambda r0, r1: intersects_seq(r0, r1, k), lambda r, rate: r.cdtime + k * r.size / rate):
		insoptimalutil(rate, "SEQ (" + str(k) + ")", util)

for k in [100]:
	for rate, util in getbestutils(lambda r0, r1: intersects_dist(r0, r1, k), lambda r, rate: r.ctime + (k + 1) * r.size / rate + k * r.dtime):
		insoptimalutil(rate, "DIST (" + str(k) + ")", util)

#and print it
optimaltable.sort()
optimaltable.print("\nBest compression utility depending on transmission speed and mode\n"
	"PAR:     Compression, Transmission and Decompression occur parallel (e.g. via netcat)\n"
	"SEQ(k):  The file is first compressed, then transmitted k times, then decompressed (e.g. via http)\n"
	"DIST(k): The file is compressed and uploaded, then downloaded and decompressed k times", "")
if args.csv:
	optimaltable.to_csv("utils-best.csv")

#print list of nonfatal errors that have occured
print_nonfatals()
