#!/usr/bin/python3
#SFT-Tech ultimate compression utility debate resolver
#
#(c) 2013	Michael En√ülin	(michael@ensslin.cc)
#
#License: GPLv3 or higher, no warranty, blabla

import os
import sys
import argparse
import sqlite3

from util import colprint, fatal, print_nonfatals, nonfatal, info, run_command, resulttable

class resultcache:
	def __init__(self, dbfile):
		#open database connection
		self.con = sqlite3.connect(dbfile)
		self.cur = self.con.cursor()
		#initialize databse
		self.cur.execute("CREATE TABLE IF NOT EXISTS results (srcfile TEXT, util TEXT, ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP, size INT, ctime REAL, dtime REAL, PRIMARY KEY(srcfile, util))")
		self.con.commit()
	
	def write(self, util, ctime, dtime, size):
		ctime = str(ctime)
		dtime = str(dtime)
		size = str(size)
		#try to delete (if exists)
		self.cur.execute("DELETE FROM results WHERE srcfile=? AND util=?", (srcfile, util))
		#insert new value
		self.cur.execute("INSERT INTO results (srcfile, util, size, ctime, dtime) VALUES (?, ?, ?, ?, ?)", (srcfile, util, size, ctime, dtime))
		self.con.commit()

	def read(self, util):
		(date, ctime, dtime, size) = self.cur.execute('SELECT ts, ctime, dtime, size FROM results WHERE srcfile=? AND util=?', (srcfile, util)).fetchone()
		return (date, float(ctime), float(dtime), int(size))
	
#parse argv
parser = argparse.ArgumentParser(description='Compression utility benchmark')
parser.add_argument("source", help="this file is used as input for benchmarking, and should contain representative data")
parser.add_argument("dest", help="compressed data is written here, the file must not exist, but the directory must be writable")
parser.add_argument("util", help="compression utility that should be benchmarked (may include arguments such as -1). if none are specified, the default utils are used.", nargs='*')
parser.add_argument("--default-utils", help="Enforce using the default utils, in addition to the manually specified ones", action="store_true", default=False)
parser.add_argument("--csv", help="Store the tables in .csv files in the current working directory", action="store_true", default=False)
parser.add_argument("--rerun", help="Do not use previously cached test results", action="store_true", default=False)
parser.add_argument("--cachefile", help="Specify cache database name", default="compbench_cache.sqlite")
args = parser.parse_args()
srcfile = os.path.abspath(args.source)
destfile = os.path.abspath(args.dest)
use_cache = not args.rerun
cache = resultcache(args.cachefile)

#compose util list
utils = args.util
if len(utils) == 0 or args.default_utils:
	#default utils
	utils += ["compress", ("zpipe -1", "zpipe -d"), ("zpipe -2", "zpipe -d"), ("zpipe -3", "zpipe -d")]
	for util in ["lzop", "gzip", "bzip2", "lzma", "xz", "xz -e"]:
		for x in range(1, 10):
			utils.append(util + " -" + str(x))

#srcfile must be a file
if not os.path.isfile(srcfile):
	if os.path.isdir(srcfile):
		print("If you wish to benchmark with the content of a directory, run it through tar -c first.")
	fatal(srcfile + " is not a file")

#destfile must be in a writable directory
destdir = os.path.dirname(destfile)
if not os.path.isdir(destdir):
	fatal("Destination file must be in an existing directory")

if not os.access(destdir, os.W_OK):
	fatal(destdir + " is not writable")

if os.path.isfile(destfile) and not os.access(destfile, os.W_OK):
	fatal(destfile + " is not writable")

#get size of srcfile
oldsize = os.path.getsize(srcfile)
if oldsize == 0:
	fatal("Source file must be non-zero sized")

class datarate():
	def __init__(self, dr):
		self.dr = dr

	def __repr__(self):
		def fourdigits(x):
			if x < 10:
				return '{:5.3f}'.format(x)
			elif x < 100:
				return '{:5.2f}'.format(x)
			elif x < 1000:
				return '{:5.1f}'.format(x)
			else:
				return '{:5.0f}'.format(x)

		if self.dr == float("inf"):
			return "+ inf  B/s"
		elif self.dr < 1000:
			return fourdigits(self.dr) + "  B/s"
		elif self.dr < 1000000:
			return fourdigits(self.dr/1000) + " KB/s"
		elif self.dr < 1000000000:
			return fourdigits(self.dr/1000000) + " MB/s"
		else:
			return fourdigits(self.dr/1000000000) + " GB/s"

class percentage():
	def __init__(self, pc):
		self.pc = pc

	def __repr__(self):
		return '{:5.2f}'.format(self.pc)

class utilresult:
	def __init__(self, util, ctime, dtime, size):
		self.util = util
		self.ctime = ctime
		self.dtime = dtime
		self.size = size

		#calculate some more stats
		self.cdmaxtime = max(ctime, dtime)
		self.cdtime = ctime + dtime
		self.diff = oldsize - size
		if self.ctime <= 0:
			self.crate = float("inf")
		else:
			self.crate = oldsize / self.ctime
		if self.dtime <= 0:
			self.drate = float("inf")
		else:
			self.drate = oldsize / self.dtime

		self.percentage = 100 * self.diff / oldsize
		self.outperformedby = None

	#returns most important members as key/value pairs
	def kvpairs(self):
		yield ("Util", self.util)
		yield ("Percentage", percentage(self.percentage))
		yield ("Compression rate", datarate(self.crate))
		yield ("Decompression rate", datarate(self.drate))
		yield ("Outperformed by", self.outperformedby)

	def __repr__(self):
		maxlen = 0
		pairs = []
		for k, v in self.kvpairs():
			if len(str(k)) > maxlen:
				maxlen = len(str(k))
			pairs.append((k, v))

		line = ""
		for k, v in pairs:
			line += str(k).ljust(maxlen + 2) + str(v) + "\n"
		return line[:-1]

	#duration for sequential operation with given transmission rate and transmission count
	def time_seq(self, rate, k):
		return self.cdtime + k * self.size / rate

	#check whether this outperforms other in all respects 
	def outperforms(self, other):
		if self.size < other.size and self.ctime < other.ctime and self.dtime < other.dtime:
			return True
		else:
			return False
			
def dobench(util, srcfile, destfile):
	if type(util) == tuple:
		comp = util[0]
		decomp = util[1]
	else:
		comp = util
		decomp = util + " -d"
	info("Testing " + comp)
	if use_cache:
		try:
			#check for cached results
			(date, ctime, dtime, size) = cache.read(comp)
			info("Using cached results from " + date)
			return utilresult(comp, ctime, dtime, size)
		except:
			pass
	#measure compression times
	ctime = run_command("pv '" + srcfile + "' | " + comp + " > '" + destfile + "'")
	if ctime != None:
		dtime = run_command("pv '" + destfile + "' | " + decomp + " > /dev/null")
	if ctime == None or dtime == None:
		nonfatal("Measurement for util " + comp + " failed")
		return None
	else:
		size = os.path.getsize(destfile)
		cache.write(comp, ctime, dtime, size)
		#return the measurement result object
		return utilresult(comp, ctime, dtime, size)

#add dummy result for 'no compression'
results = [utilresult("cat", 0, 0, oldsize)]
for r in map(lambda util: dobench(util, srcfile, destfile), utils):
	if r == None:
		continue

	#check whether this result outperforms/is outperformed by any other existing result
	for other in results:
		if r.outperforms(other):
			#r is better than other in all aspects
			other.outperformedby = r.util

		if other.outperforms(r):
			#r is worse than other in all aspects
			r.outperformedby = other.util

	#append result to result list
	results.append(r)

	#print result immediately, for the impatient users
	print(r)

print("Processing measurement results...")

#sort the results by their achieved compression rates
results = sorted(results, key=lambda r: r.percentage)

#takes a list of unsorted points, and returns a yields interval tuples in a sorted order.
#the interval tuples consist of (start, middle, end).
#the last interval tuple will be (start, +inf, +inf).
def list_to_intervals(l):
	lastpoint = None
	for point in sorted(l) + [float("inf")]:
		if lastpoint == point:
			continue
		if lastpoint != None:
			yield (lastpoint, (point + lastpoint)/2, point)
		lastpoint = point

#yields (datarate, best util) tuples
#intersectfun must return a list of (possible) intersections
#between the time curves of two utilresults
#timefun must return the time for a given rate
def getbestutils(intersectfun, timefun):
	#find the util that has the highest compression (rate = 0)
	curbest = results[0]
	for r in results:
		if(r.size < curbest.size):
			curbest = r
	yield (0, curbest)

	intersections = []
	for r0 in results:
		for r1 in results:
			inters = intersectfun(r0, r1)
			if type(inters) == list:
				intersections += inters
			else:
				intersections.append(inters)

	#begin is the rate at the beginning of the rate interval,
	#middle is the rate in the middle of the interval
	#we do not need the rate at the end.
	for begin, middle, _ in list_to_intervals(intersections):
		#ignore intersections at negative rates
		if begin < 0:
			continue

		oldbest = curbest
		for r in results:
			#check whether r is better than curbest in the
			#given interval
			if timefun(r, middle) < timefun(curbest, middle):
				curbest = r
				
		#check whether curbest has changed
		if curbest != oldbest:
			yield (begin, curbest)

#create table for measurement results
utilresults = resulttable()
utilresults_outperformed = resulttable()
for r in results:
	if(r.outperformedby == None):
		table = utilresults
	else:
		table = utilresults_outperformed
	#insert r into the table
	for k, v in r.kvpairs():
		table.insval(r.util, k, v)
#and print it
utilresults.print("\n\nMeasurement results for compression utilities")
if len(utilresults_outperformed.rows) > 0:
	utilresults_outperformed.print("\nCompression utilities that are worse than some other in all aspects, i.e. completely useless for this workload")

if args.csv:
	utilresults.to_csv("utils.csv")
	utilresults_outperformed.to_csv("utils-outperformed.csv")

#helper functions for find best utils for given datarates
#these functions find intersections between the time(rate) cuves of two
#utils r0 and r1.

def intersects_par(r0, r1):
	#intersections can only occur between saturated and non-saturated
	#lines.

	#r0.cdmaxtime = r1.size/rate
	#r1.cdmaxtime = r0.size/rate
	result = []
	if r0.cdmaxtime > 0:
		result.append(r1.size/r0.cdmaxtime)
	if r1.cdmaxtime > 0:
		result.append(r0.size/r1.cdmaxtime)
	return result

def intersects_seq(r0, r1, k):
	#r0.cdtime + r0.size/rate = r1.cdtime + r1.size/rate
	cdtimediff = r1.cdtime - r0.cdtime
	sizediff = r0.size - r1.size
	#no dividing by zero
	if cdtimediff != 0:
		return k * sizediff / cdtimediff
	else:
		return []

def intersects_dist(r0, r1, k):
	#r0.ctime + (k + 1) * r0.size/rate + k * r0.dtime =
	#r1.ctime + (k + 1) * r1.size/rate + k * r1.dtime
	r0cddtime = r0.ctime + k * r0.dtime
	r1cddtime = r1.ctime + k * r1.dtime
	cddtimediff = r1cddtime - r0cddtime
	sizediff = r0.size - r1.size
	#no dividing by zero
	if cddtimediff != 0:
		return (k + 1) * sizediff / cddtimediff
	else:
		return []

#create table of optimal utils for datarates
optimal_utils = resulttable()
def insoptimalutil(rate, col, util):
	optimal_utils.insval(rate, "Data rate", ">= " + str(datarate(rate)))
	optimal_utils.insval(rate, col, util.util)

for rate, util in getbestutils(intersects_par, lambda r, rate: max(r.cdmaxtime, r.size/rate)):
	insoptimalutil(rate, "PAR", util)

for k in [1, 2]:
	for rate, util in getbestutils(lambda r0, r1: intersects_seq(r0, r1, k), lambda r, rate: r.cdtime + k * r.size / rate):
		insoptimalutil(rate, "SEQ (" + str(k) + ")", util)

for k in [100]:
	for rate, util in getbestutils(lambda r0, r1: intersects_dist(r0, r1, k), lambda r, rate: r.ctime + (k + 1) * r.size / rate + k * r.dtime):
		insoptimalutil(rate, "DIST (" + str(k) + ")", util)

#and print it
optimal_utils.sort()
optimal_utils.print("\nBest compression utility depending on transmission speed and mode\n"
	"PAR:     Compression, Transmission and Decompression occur parallel (e.g. via netcat)\n"
	"SEQ(k):  The file is first compressed, then transmitted k times, then decompressed (e.g. via http)\n"
	"DIST(k): The file is compressed and uploaded, then downloaded and decompressed k times", "")
if args.csv:
	optimal_utils.to_csv("utils-best.csv")

#print list of nonfatal errors that have occured
print_nonfatals()
