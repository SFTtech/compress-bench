#!/usr/bin/python3
#SFT-Tech ultimate compression utility debate resolver
#
#(c) 2013	Michael Enßlin	(michael@ensslin.cc)
#
#License: GPLv3 or higher, no warranty, blabla

import os
import sys
import collections
import argparse

def colprint(msg, colcode):
	print("\x1b[" + str(colcode) + "m" + msg + "\x1b[m")

def fatal(msg):
	colprint(msg, 31)
	sys.exit(1)

#store all non-fatal errors
nonfatals = []

def nonfatal(msg):
	colprint(msg, 33)
	nonfatals.append(msg)

def info(msg):
	colprint(msg, 36)

#parse argv
parser = argparse.ArgumentParser(description='Compression utility benchmark')
parser.add_argument("source", help="this file is used as input for benchmarking, and should contain representative data")
parser.add_argument("dest", help="compressed data is written here, the file must not exist, but the directory must be writable")
parser.add_argument("util", help="compression utility that should be benchmarked (may include arguments such as -1). if none are specified, the default utils are used.", nargs='*')
parser.add_argument("--defaultutils", help="Enforce using the default utils, in addition to the manually specified ones", action="store_true", default=False)
parser.add_argument("--csv", help="Store the tables in .csv files in the current working directory", action="store_true", default=False)
args = parser.parse_args()
sourcefile = os.path.abspath(args.source)
destfile = os.path.abspath(args.dest)

#compose util list
utils = args.util
if len(utils) == 0 or args.defaultutils:
	#default utils
	utils += ["compress", ("zpipe -1", "zpipe -d"), ("zpipe -2", "zpipe -d"), ("zpipe -3", "zpipe -d")]
	for util in ["lzop", "gzip", "bzip2", "lzma", "xz", "xz -e"]:
		for x in range(1, 10):
			utils.append(util + " -" + str(x))

#sourcefile must be a file
if not os.path.isfile(sourcefile):
	if os.path.isdir(sourcefile):
		print("If you wish to benchmark with the content of a directory, run it through tar -c first.")
	fatal(sourcefile + " is not a file")

#destfile must be in a writable directory
destdir = os.path.dirname(destfile)
if not os.path.isdir(destdir):
	fatal("Destination file must be in an existing directory")

if not os.access(destdir, os.W_OK):
	fatal(destdir + " is not writable")

if os.path.isfile(destfile) and not os.access(destfile, os.W_OK):
	fatal(destfile + " is not writable")

#get size of sourcefile
oldsize = os.path.getsize(sourcefile)
if oldsize == 0:
	fatal("Source file must be non-zero sized")

class datarate():
	def __init__(self, dr):
		self.dr = dr

	def __repr__(self):
		def fourdigits(x):
			if x < 10:
				return '{:5.3f}'.format(x)
			elif x < 100:
				return '{:5.2f}'.format(x)
			elif x < 1000:
				return '{:5.1f}'.format(x)
			else:
				return '{:5.0f}'.format(x)

		if self.dr == float("inf"):
			return "+ inf  B/s"
		elif self.dr < 1000:
			return fourdigits(self.dr) + "  B/s"
		elif self.dr < 1000000:
			return fourdigits(self.dr/1000) + " KB/s"
		elif self.dr < 1000000000:
			return fourdigits(self.dr/1000000) + " MB/s"
		else:
			return fourdigits(self.dr/1000000000) + " GB/s"

class percentage():
	def __init__(self, pc):
		self.pc = pc

	def __repr__(self):
		return '{:5.2f}'.format(self.pc)

class utilresult:
	def __init__(self, util, ctime, dtime, size):
		self.util = util
		self.ctime = ctime
		self.dtime = dtime
		self.size = size

		#calculate some more stats
		self.cdmaxtime = max(ctime, dtime)
		self.cdtime = ctime + dtime
		self.diff = oldsize - size
		if self.ctime <= 0:
			self.crate = float("inf")
		else:
			self.crate = oldsize / self.ctime
		if self.dtime <= 0:
			self.drate = float("inf")
		else:
			self.drate = oldsize / self.dtime

		self.percentage = 100 * self.diff / oldsize
		self.outperformedby = None

	#returns most important members as key/value pairs
	def kvpairs(self):
		yield("Util", self.util)
		yield("Percentage", percentage(self.percentage))
		yield("Compression rate", datarate(self.crate))
		yield("Decompression rate", datarate(self.drate))
		yield("Outperformed by", self.outperformedby)

	def __repr__(self):
		maxlen = 0
		pairs = []
		for k, v in self.kvpairs():
			if len(str(k)) > maxlen:
				maxlen = len(str(k))
			pairs.append((k, v))

		line = ""
		for k, v in pairs:
			line += str(k).ljust(maxlen + 2) + str(v) + "\n"
		return line[:-1]

	#duration for parallel operation with given transmission rate
	def time_par(self, rate):
		return max(self.cdmaxtime, self.size / rate)

	#duration for sequential operation with given transmission rate and transmission count
	def time_seq(self, rate, k):
		return self.cdtime + k * self.size / rate

	#check whether this outperforms other in all respects 
	def outperforms(self, other):
		if self.size < other.size and self.ctime < other.ctime and self.dtime < other.dtime:
			return True
		else:
			return False
			
#monotonic clock code
import ctypes
__all__ = ["monotonic_time"]
CLOCK_MONOTONIC = 1 # see <linux/time.h>
class timespec(ctypes.Structure):
    _fields_ = [
        ('tv_sec', ctypes.c_long),
        ('tv_nsec', ctypes.c_long)
    ]

librt = ctypes.CDLL('librt.so.1', use_errno=True)
clock_gettime = librt.clock_gettime
clock_gettime.argtypes = [ctypes.c_int, ctypes.POINTER(timespec)]

#returns monotonic time, as float, in seconds
def monotonic_time():
    t = timespec()
    if clock_gettime(CLOCK_MONOTONIC, ctypes.pointer(t)) != 0:
        errno_ = ctypes.get_errno()
        raise OSError(errno_, os.strerror(errno_))
    return t.tv_sec + t.tv_nsec * 1e-9

#returns runtime of command, as float, in seconds
def timecommand(command):
	print(command)
	t_begin = monotonic_time()
	exitcode = os.system(command)
	t_end = monotonic_time()

	if exitcode == 0:
		return t_end - t_begin
	else:
		nonfatal("command \"" + command + "\" failed")
		return None

def dobench(util, sourcefile, destfile):
	if type(util) == tuple:
		comp = util[0]
		decomp = util[1]
	else:
		comp = util
		decomp = util + " -d"
	info("Testing " + comp)
	#measure compression times
	ctime = timecommand("pv '" + sourcefile + "' | " + comp + " > '" + destfile + "'")
	if ctime != None:
		dtime = timecommand("pv '" + destfile + "' | " + decomp + " > /dev/null")
	if ctime == None or dtime == None:
		nonfatal("Measurement for util " + comp + " failed")
		return None
	else:
		#return the measurement result object
		return utilresult(comp, ctime, dtime, os.path.getsize(destfile))

results = [utilresult("cat", 0, 0, oldsize)]
for r in map(lambda util: dobench(util, sourcefile, destfile), utils):
	if r == None:
		continue

	#check whether this result outperforms/is outperformed by any other existing result
	for other in results:
		if r.outperforms(other):
			#r is better than other in all aspects
			other.outperformedby = r.util

		if other.outperforms(r):
			#r is worse than other in all aspects
			r.outperformedby = other.util

	#append result to result list
	results.append(r)

	#print result immediately, for the impatient users
	print(r)

print("Processing measurement results...")

#sort the results by their achieved compression rates
results = sorted(results, key=lambda r: r.percentage)

#takes a list of unsorted points, and returns a yields interval tuples in a sorted order.
#the interval tuples consist of (start, middle, end).
#the last interval tuple will be (start, +inf, +inf).
def list_to_intervals(l):
	lastpoint = None
	for point in sorted(l) + [float("inf")]:
		if lastpoint == point:
			continue
		if lastpoint != None:
			yield (lastpoint, (point + lastpoint)/2, point)
		lastpoint = point

#for each transmission rate, find the util that is optimal for PARALLEL operation
#t(rate) = max(t_transmission(rate), cdmaxtime) = max(size/rate, cdmaxtime)
#when plotting 1/t(rate), we need to find the curve that is highest
#curves can only switch positions at places where intersections occur, and these occur only between saturated and non-saturated curve parts

#find the util that has the highest compression for rate = 0
curbestutil = results[0]
for r in results:
	if(r.size < curbestutil.size):
		curbestutil = r
#we generate a result list that has (rate, best_util) tuples
bestutil_par = [(0, curbestutil)]

#when plotting the 1/t(rate) curves for all results against rate, we need to find the curve that is highest
#those curves switch only at places where intersections occur, and only saturated and non-saturated curves can intersect
#we're lazy, just generate the O(n²) list of possible intersections.
intersections = []
for r0 in results:
	for r1 in results:
		#possible intersection points are where one of these formula are true
		#	1/r0.cdmaxtime = rate/r1.size
		#	1/r1.cdmaxtime = rate/r0.size
		if r0.cdmaxtime > 0:
			intersections.append(r1.size/r0.cdmaxtime)
		if r1.cdmaxtime > 0:
			intersections.append(r0.size/r1.cdmaxtime)

#we're lazy as fuck, this brute-force algorithm takes O(n³) time, where O(nlogn) would be possible...
for rate_begin, rate_middle, rate_end in list_to_intervals(intersections):
	#ignore intersections at negative rates
	if rate_begin < 0:
		continue

	oldbestutil = curbestutil
	curbestutil = results[0]
	for r in results:
		#check whether r is better than curbestutil
		if r.time_par(rate_middle) < curbestutil.time_par(rate_middle):
			curbestutil = r
	if curbestutil != oldbestutil:
		bestutil_par.append((rate_begin, curbestutil))

#for each transmission rate, find the util that is optimal for SEQUENTIAL operation with k-fold tansmission
#t(rate, k) = k * t_transmission(rate) + cdtime = k * size/rate + cdtime
#when plotting t(rate, k), we need to find the curve that is lowest

#find the util that has the highest compression for rate = 0
curbestutil = results[0]
for r in results:
	if(r.size < curbestutil.size):
		curbestutil = r
#we generate result lists for different values of k
bestutil_seq = collections.OrderedDict()
bestutil_seq_ks = [1, 2, 5]
for k in bestutil_seq_ks:
	bestutil_seq[k] = [(0, curbestutil)]

for k in bestutil_seq_ks:
	#again, the O(n²) intersection algorithm....
	intersections = []
	for r0 in results:
		for r1 in results:
			cdtimediff = r1.cdtime - r0.cdtime
			sizediff = r0.size - r1.size

			#we only calculate intersections if the times differ by more than 0.01 percent.
			#else, the intersection would be close to infinity, and thus irrelevant.
			#(avoiding division by zero errors)
			if(abs(cdtimediff) > 0.0001 * (r1.cdtime)):
				intersections.append(k * sizediff / cdtimediff)

	#again, the O(n³) brute-force algorithm
	curbestutil = bestutil_seq[k][0][1]
	for rate_begin, rate_middle, rate_end in list_to_intervals(intersections):
		#ignore negative intersections
		if rate_begin < 0:
			continue

		oldbestutil = curbestutil
		curbestutil = results[0]
		for r in results:
			#check whether r is better than curbestutil
			if r.time_seq(rate_middle, k) < curbestutil.time_seq(rate_middle, k):
				curbestutil = r
		if curbestutil != oldbestutil:
			bestutil_seq[k].append((rate_begin, curbestutil))

class resulttable:
	def __init__(self):
		self.data = {}
		self.rows = collections.OrderedDict()
		self.cols = collections.OrderedDict()

	def insval(self, row, col, v):
		if v == None:
			return

		#update column and row indices with their maximum field size values
		if not row in self.rows:
			self.rows[row] = max(len(str(row)), len(str(v)))
			self.data[row] = collections.OrderedDict()
		else:
			self.rows[row] = max(self.rows[row], len(str(v)))

		if not col in self.cols:
			self.cols[col] = max(len(str(col)), len(str(v)))
		else:
			self.cols[col] = max(self.cols[col], len(str(v)))

		#store the value
		self.data[row][col] = v

	def getval(self, row, col):
		if not row in self.rows or not col in self.data[row]:
			return None
		else:
			return self.data[row][col]

	def getvalstr(self, row, col):
		if not col in self.cols:
			width = 0
		else:
			width = self.cols[col]

		return str(self.getval(row, col)).ljust(width)

	def getline(self, source, sep, padding):
		line = ""
		for col in self.cols:
			data = str(source(col))
			if padding:
				line += data.ljust(self.cols[col])
			else:
				line += data
			line += sep

		return line[:-len(sep)]

	def gettitle(self, sep = " | ", padding = True):
		return self.getline(lambda x: x, sep, padding)

	def getempty(self, sep = " | ", padding = True):
		return self.getline(lambda x: "", sep, padding)

	def getbody(self, sep = " | ", padding = True):
		body = ""
		for row in self.rows:
			body += self.getline(lambda x: self.getvalstr(row, x), sep, padding) + "\n"
		return body[:-1]

	def print(self, desc):
		print(desc + "\n")
		print(self.gettitle())
		print(self.getempty())
		print(self.getbody())
	
	def to_csv(self, filename):
		f = open(filename, 'w')
		f.write(self.gettitle(",") + "\n")
		f.write(self.getbody(",") + "\n")
		f.close()

#create table for measurement results
goodtable = resulttable()
badtable = resulttable()
for r in results:
	if(r.outperformedby == None):
		table = goodtable
	else:
		table = badtable
	#insert r into the table
	for k, v in r.kvpairs():
		table.insval(r.util, k, v)
#and print it
goodtable.print("\n\nMeasurement results for compression utilities")
if len(badtable.rows) > 0:
	badtable.print("\nCompression utilities that are worse than some other in all aspects, i.e. completely useless for this workload")

if args.csv:
	goodtable.to_csv("utils.csv")
	badtable.to_csv("utils-outperformed.csv")

#create table for optimal comps for datarates
table = resulttable()
#get all datarate intervals
all_points = bestutil_par
for k in bestutil_seq_ks:
	all_points = all_points + bestutil_seq[k]


lastinserted = None
for begin, middle, end in list_to_intervals(map(lambda x: x[0], all_points)):
	#get the optimal values for bestutil_par and bestutil_seq[k]
	def find_matching(l, pos):
		for k, v in reversed(l):
			if k <= pos:
				return v.util

	#only insert a new tuple if any of the values have changed, else it would not make sense to add an other row
	inslist = [("PAR", find_matching(bestutil_par, middle))]
	inslist += map(lambda k: ("SEQ (" + str(k) + ")", find_matching(bestutil_seq[k], middle)), bestutil_seq_ks)

	if(inslist != lastinserted):
		table.insval(begin, "Data rate", ">= " + str(datarate(begin)))
		for k, v in inslist:
			table.insval(begin, k, v)
	lastinserted = inslist

#and print it
table.print("\nBest compression utility depending on transmission speed and mode\n"
	"PAR:    Compression, Transmission and Decompression occur parallel (e.g. via netcat)\n"
	"SEQ(k): The file is first compressed, then transmitted k times, then decompressed (e.g. via http)")
if args.csv:
	table.to_csv("utils-best.csv")

if len(nonfatals) > 0:
	print("\nNonfatal errors:")
	for nonfatal in nonfatals:
		print(nonfatal)

